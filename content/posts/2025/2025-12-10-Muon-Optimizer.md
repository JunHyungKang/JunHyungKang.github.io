---
title: "이제는 AdamW가 아니라 Muon 이라고?!"
date: "2025-12-10"
teaser: "Kimi K2와 1조 파라미터 모델이 선택한 새로운 최적화 기법, Muon Optimizer에 대해 알아봅니다. AdamW의 한계를 넘어 행렬 직교화를 통해 학습 효율과 안정성을 동시에 잡은 혁신적인 방법을 소개합니다."
tags:
  - Deep Learning
  - Optimizer
  - Muon
  - Kimi K2
  - LLM
  - AdamW
---
![Muon Optimizer Visualization](/images/posts/2025/2025-12-10-Muon-Optimizer/header.png)

지난 10년간 대규모 언어 모델(LLM) 학습 분야는 **아담(Adam)** 옵티마이저의 지배 아래 있었다 해도 과언이 아닙니다.

![Gradient Descent와 Adam Optimizer 개념도](/images/posts/2025/2025-12-10-Muon-Optimizer/gradient_descent.png)
<div align="center" style="font-size: 0.8em; color: gray;">(이미지 출처: <a href="https://www.youtube.com/watch?v=bO5nvE289ec">YouTube</a>)</div>

우리는 **경사 하강법(Gradient Descent)**을 이미 잘 알고 있습니다. 눈을 가린 채 산에서 가장 낮은 곳으로 내려가기 위해, 현재 발밑의 경사가 가장 가파른 방향으로 한 걸음씩 내딛는 과정이죠. 이 단순하고도 강력한 개념은 SGD(Stochastic Gradient Descent)로 발전했고, 여기에 '관성'을 더한 Momentum, 보폭을 조절하는 RMSProp 등이 더해지며 진화해왔습니다.

그리고 이 모든 장점을 집대성한 것이 바로 **Adam(Adaptive Moment Estimation)**입니다. 제가 대학원을 졸업할 당시, 모든 논문의 실험 섹션에는 **"We used AdamW optimizer..."**라는 문장이 관용구처럼 적혀있었습니다. AdamW는 Adam에 가중치 감쇠(Weight Decay)를 올바르게 적용하여 일반화 성능을 높인 버전으로, 그야말로 '대안이 없는 표준(De Facto Standard)'이었죠.

하지만 모델의 규모가 수천억 개를 넘어 1조(Trillion) 개의 파라미터로 확장되면서, 이 견고했던 아담의 왕좌가 흔들리기 시작했습니다. 기존 아담의 한계가 명확하게 드러나기 시작한 것입니다.

*   **메모리 비효율성:** 아담은 각 파라미터마다 1차 모멘텀과 2차 모멘텀이라는 **두 개의 상태 변수를 추가로 저장**해야 하므로, 메모리 사용량이 모델 파라미터 자체의 **두 배**에 달합니다.
*   **저차원 업데이트:** 아담 방식의 업데이트는 그래디언트의 크기가 큰 소수의 '지배적인 방향'에만 집중되어, 마치 거대한 행렬 파라미터가 있어도 실제로는 **일부 차원만 의미 있게 바뀌는** '저차원 행렬(low-rank matrix)'처럼 작용하는 경향이 있습니다. 이는 파라미터의 잠재력을 완전히 활용하지 못하고 학습 효율(토큰 효율성)을 저해합니다.


사실 저 또한 이러한 내용을 전혀 모르고 있었습니다. 하지만 최근 **Kimi K2** 테크니컬 리포트를 읽으며 **뮤온(Muon)**이라는 새로운 최적화 기법을 처음 접하게 되었고, 이 혁신적인 내용을 공유하고 싶어 이 글을 작성하게 되었습니다.

---

### 제1부: 뮤온(Muon)의 핵심 철학, '행렬 직교화'

![직교화 개념: 편향된 벡터 업데이트 vs 균형 잡힌 행렬 직교화](/images/posts/2025/2025-12-10-Muon-Optimizer/orthogonalization.png)

뮤온 옵티마이저의 정식 명칭은 **MomentUm Orthogonalized by Newton-Schulz**입니다. 이 이름에 뮤온의 혁신적인 철학이 모두 담겨 있습니다.

Muon은 기존 옵티마이저가 파라미터를 단순한 '벡터'로 취급했던 것과 달리, Linear layer의 가중치와 같은 2차원 파라미터(행렬)의 **구조 자체**에 주목합니다.

뮤온의 핵심 아이디어는 **'직교화(Orthogonalization)'**입니다.

저차원 업데이트의 문제를 해결하기 위해, 뮤온은 모멘텀 행렬을 **직교 행렬(orthogonal matrix)**과 유사하게 변환합니다. 이 변환을 통해 업데이트가 특정 방향에 편향되지 않고 **모든 차원에 걸쳐 균일하게 분산**되도록 만듭니다. 그 결과, 기존 옵티마이저에서 무시되기 쉬웠던 **'드물게 중요한 학습 방향(rare directions)'**의 효과가 증폭되어, 모델이 더 정교하고 미묘한 패턴까지 학습할 수 있게 됩니다.

---

### 제2부: SVD를 대체하는 놀라운 효율, 뉴턴-슐츠 반복

![뉴턴-슐츠 반복 수렴 그래프 (Muon 논문 계수 기반 재현)](/images/posts/2025/2025-12-10-Muon-Optimizer/newton-schulz.png)

이론적으로 행렬을 직교화하는 가장 정확한 방법은 **SVD(특이값 분해)**를 이용해 $UV^\top$ 행렬을 얻는 것입니다. 하지만 SVD 연산은 **계산 비용이 너무 커서** 매 학습 스텝마다 적용하기에는 비현실적입니다.

뮤온은 이 문제를 해결하기 위해 **뉴턴-슐츠 반복(Newton-Schulz iteration)**이라는 방법을 사용합니다.

1. **모멘텀 계산:** 먼저 SGD 모멘텀(Momentum) 방식으로 기본적인 업데이트 방향을 계산합니다.
2. **다항식 근사:** 이 모멘텀 행렬에 **홀수차 다항식 함수**($\varphi(x) = ax + bx^3 + cx^5$)를 반복적으로 적용합니다.
3. **직교화:** 이 반복 과정을 (보통) **5번** 수행하면, 행렬의 특이값들이 모두 1에 가깝게 수렴하면서 SVD와 유사한 **직교화 효과**를 얻게 됩니다.

이 뉴턴-슐츠 반복의 가장 큰 장점은 **오직 행렬 곱셈만으로 구성**되어 있어 GPU에서 매우 빠르게 처리될 수 있다는 점입니다. 이 덕분에 뮤온은 아담W 대비 전체 학습 시간을 단축하면서도 FLOP 오버헤드는 **1% 미만**으로 유지할 수 있습니다.

**실용적인 설계:** 뮤온은 직교화가 의미 있는 행렬(2D) 파라미터에만 적용되며, 임베딩이나 편향(bias) 같은 1차원(1D) 파라미터는 기존의 **AdamW**를 사용하는 **하이브리드 전략**을 취합니다.

---

### 제3부: 대규모 학습의 안전장치, MuonClip과 QK-Clip

뮤온은 높은 토큰 효율성을 제공하지만, 모든 방향에 균일하게 힘을 가하는 고차원 업데이트 방식 때문에 대규모 모델 학습 시 **치명적인 불안정성**을 야기했습니다.

이 문제는 주로 트랜스포머의 어텐션 레이어에서 **어텐션 로짓(attention logits)** 값이 비정상적으로 폭발적으로 커지는 현상으로 나타납니다. 이는 손실(loss)을 무한대로 치솟게 하여 학습을 완전히 실패(divergence)하게 만듭니다. 이는 마치 모든 바퀴에 최대 출력을 가하려다 자동차가 균형을 잃고 스핀하는 것과 같습니다.

![QK-Clip 적용에 따른 어텐션 로짓 안정성 시뮬레이션 (Kimi K2 테크니컬 리포트 기반 재현)](/images/posts/2025/2025-12-10-Muon-Optimizer/qk-clip.png)

Moonshot AI는 이 문제를 해결하기 위해 **QK-Clip**이라는 새로운 메커니즘을 제안했습니다.

**QK-Clip의 작동 원리:**
1.  **모니터링:** 매 학습 스텝마다, 각 어텐션 헤드에서 계산된 어텐션 로짓의 최댓값($S_{h}^{\max}$)을 감시합니다.
2.  **조건부 개입:** 이 최댓값이 설정된 임계값($\tau$, Kimi K2의 경우 100)을 초과하는 경우에만 개입합니다.
3.  **원인 제어:** QK-Clip은 로짓을 직접 자르는 대신, 로짓 폭발의 근본 원인인 **쿼리($W_Q$) 및 키($W_K$) 투영 가중치 행렬**의 크기를 직접 **재조정(rescaling)**하여 성장을 억제합니다.

Muon 옵티마이저에 QK-Clip과 기존의 안정화 기법(가중치 감소, RMS 스케일링)을 통합한 것이 바로 **MuonClip**입니다. 이 안전장치 덕분에 Kimi K2는 **15.5조 토큰**이라는 방대한 양을 학습하는 동안 **단 한 번의 손실 스파이크 없이** 안정적인 학습을 완료할 수 있었습니다.

---

### 결론: AdamW는 '만능 세단', MuonClip은 '레이싱 머신'

그렇다면 엔지니어의 입장에서 어떤 옵티마이저를 선택해야 할까요?

| 특징 구분 | AdamW | Muon (Vanilla) | MuonClip |
| :--- | :--- | :--- | :--- |
| **핵심 철학** | 적응형 모멘트 추정 (벡터 기반) | **행렬 직교화** (고차원 업데이트) | Muon + 안정화 기술 |
| **메모리 효율** | 파라미터 대비 2배의 상태 메모리 | SGD-모멘텀 기반 (AdamW보다 효율적) | SGD-모멘텀 기반 (AdamW보다 효율적) |
| **토큰 효율성** | 낮음 (저차원 업데이트) | **매우 높음** | **매우 높음** |
| **대규모 학습 안정성** | **매우 안정적** | 불안정 (어텐션 로짓 폭발) | **매우 안정적** (QK-Clip 적용) |

**AdamW**는 여전히 파인튜닝이나 안정성이 최우선인 프로젝트에서 믿음직한 **'만능 세단'**과 같습니다.

하지만 **10억 개 이상의 파라미터**를 가진 대규모 LLM을 처음부터 사전 학습하는 **'F1 그랑프리'**와 같은 극한의 환경에서는, **MuonClip**이라는 **'특수 제작된 레이싱 머신'**이 압도적인 토큰 효율성과 함께 놀라운 안정성을 제공합니다. 1.5B 파라미터 모델 훈련 시, AdamW가 13.3시간 걸린 작업을 Muon은 **10시간 만에** 끝내는 속도 차이가 이를 증명합니다.

뮤온클립의 등장은 단순히 옵티마이저의 성능을 개선한 것을 넘어, LLM 학습 비용을 절감하고 거대 모델 개발의 문턱을 낮추는 중요한 기술적 변곡점이 될 것입니다. 모델 파라미터의 내부 **행렬 구조(matrix structure)**에 집중하는 것이 최적화의 새로운 트렌드임을 기억하고, 다음 프로젝트에서 이 강력한 도전을 활용해보시길 바랍니다.

---

### 참고 자료

*   [Muon: Only 1% overhead for >30% training cost savings (Keller Jordan)](https://kellerjordan.github.io/posts/muon/)
*   [New Optimizer for LLMs - Muon (YouTube)](https://www.youtube.com/watch?v=bO5nvE289ec)

